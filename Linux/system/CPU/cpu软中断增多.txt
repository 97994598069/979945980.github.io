中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力
由于中断处理程序会打断其他进程的运行，所以，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行。如果中断本身要做的事情不多，那么处理起来也不会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间

特别是，中断处理程序在响应中断时，还会临时关闭中断，这就会导致上一次中断处理完成之前，其他中断都不用时时等待着

事实上，为了解决中断处理程序执行过长和中断丢失的问题，Linux将中断处理过程分成了两个阶段，也就是上半部和下半部：
上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作
下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行

举例：
网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。
对于上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了），最后再发送一个软中断信号，通知下半部做进一步的处理

所以，这两个阶段也可以如下理解：
上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行
下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行

实际上，上半部会打断cpu正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个CPU都对应一个软中断内核线程，名字未ksoftirqs/cpu编号   比如：ksoftirqs/0

软中断不仅包括了刚刚所讲的硬件设备中断处理程序的下半部，一些内核自定义的事件也属于软中断，比如内核调度和RCPU锁等等



查看软中断和内核线程：
cat /proc/softirqs提供了软中断的运行情况
/proc/interrupts提供了硬中断的运行情况

$ cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:     811613    1972736
      NET_TX:         49          7
      NET_RX:    1136736    1506885
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     304787       3691
       SCHED:     689718    1897539
     HRTIMER:          0          0
         RCU:    1330771    1354737

需注意：
1)要注意软中断的类型，也就是这个界面中第一列的内容。从第一列可以看到，软中断包括了10个类别，分别对应不同的工作类型。比如NET_RX表示网络接收中断，而NET_TX表示网络发送中断

2)要注意同一种软中断在不同CPU上的分布情况，也就是同一行的内容，正常情况下，同一种中断在不同CPU上的累计次数应该差不多，比如这个界面种NET_RX在CPU0和CPU1上的中断次数基本是同一个数量级，相差不大

不过你可能发现TASKLET在不同CPU上的分布并不均匀，TASKLET是最常用的软中断实现机制，每个TASKLET只运行一次就会结束，并且只在调用它的函数所在的CPU上运行；；因此，使用TASKLET特别简单，当然也会存在一些问题，比如说由于只在一个CPU上运行导致的调度不均衡，再比如因为不能在多个CPU上并运行带来了性能限制

$ ps aux | grep softirq
root         7  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/0]
root        16  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/1]




案例：
打开一个终端，执行如下命令：
# 运行 Nginx 服务并对外开放 80 端口
$ docker run -itd --name=nginx -p 80:80 nginx

然后，在第二个终端，使用curl访问nginx监听的端口，确认nginx正常启动。
$ curl http://192.168.0.30/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...

接着，还是在第二个终端，运行hping3命令，来模拟nginx的客户端请求：
# -S 参数表示设置 TCP 协议的 SYN（同步序列号），-p 表示目的端口为 80
# -i u100 表示每隔 100 微秒发送一个网络帧
# 注：如果你在实践过程中现象不明显，可以尝试把 100 调小，比如调成 10 甚至 1
$ hping3 -S -p 80 -i u100 192.168.0.30
##hping3命令是一个SYN FLOOD攻击


现在回到第一个终端，发现了异常；感觉系统响应明显变慢了，即便只是在终端种敲几个回车，都得很久才能得到响应，为什么？

# top 运行后按数字 1 切换到显示所有 CPU
$ top
top - 10:50:58 up 1 days, 22:10,  1 user,  load average: 0.00, 0.00, 0.00
Tasks: 122 total,   1 running,  71 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 95.6 id,  0.0 wa,  0.0 hi,  4.4 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
    7 root      20   0       0      0      0 S   0.3  0.0   0:01.64 ksoftirqd/0
   16 root      20   0       0      0      0 S   0.3  0.0   0:01.97 ksoftirqd/1
 2663 root      20   0  923480  28292  13996 S   0.3  0.3   4:58.66 docker-containe
 3699 root      20   0       0      0      0 I   0.3  0.0   0:00.13 kworker/u4:0
 3708 root      20   0   44572   4176   3512 R   0.3  0.1   0:00.07 top
    1 root      20   0  225384   9136   6724 S   0.0  0.1   0:23.25 systemd
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.03 kthreadd
...

top查看得到：
负载为0，就绪队列里面只有一个进程（1 running）
每个cpu得使用率都挺低，最高得cpu1得使用率也只有4.4%,并不高
再看进程列表，cpu使用率最高得进程也只有0.3%，也不高

但是为什么系统响应慢了呢？
仔细查看top输出得：两个cpu得使用率虽然分别只有3.3%和4.4%，但都用在了软中断上；而从进程列表上也可以看到，cpu使用率最高得也是软中断进程ksoftirqd。看起来软中断可疑了

此时我们需要查看软中断类型：
cat /proc/ksoftirqs：
不过这里各类软中断次数，又是什么时间断里的次数呢？它是系统运行依赖的累积中断次数；所以我们直接查看文件内容，得到的只是累积中断次数，对这里的问题并没有直接参考意义。因为这些软中断次数的变化速率才是我们需要关注的

观察各个类型软中断的变化速率：
$ watch -d cat /proc/softirqs
                    CPU0       CPU1
          HI:          0          0
       TIMER:    1083906    2368646
      NET_TX:         53          9
      NET_RX:    1550643    1916776
       BLOCK:          0          0
    IRQ_POLL:          0          0
     TASKLET:     333637       3930
       SCHED:     963675    2293171
     HRTIMER:          0          0
         RCU:    1542111    1590625

TIMER  定时中断
NET_RX 网络接收中断
SCHED  内核调度中断
RCU    RCU锁中断


其中NET_RX,也就是网络数据包接收软中断的变化速率最快。而其他几种类型的软中断，是保证Linux中断、时钟和临界区保护这些工作所必需的，所以他们的变化倒是正常的

# -n DEV 表示显示网络收发的报告，间隔 1 秒输出一组数据
$ sar -n DEV 1
15:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.01
15:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.00
15:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
15:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05

sar命令可以用来查看系统的网络收发情况，还有一个好处是，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的pps,即每秒收发的网络帧数

第1列：表示报告时间
第2列：IFACE表示网卡
第3、4列：表示每秒接收和发送的网络帧数，也就是pps
第5、6列，表示每秒接收和发送的千字节数，也就是BPS

分析得：
1)对于网卡eth0来说，每秒接收得网络帧数比较大，达到了12607，而发送得网络帧数则比较小，只有6304；每秒接收的千字节数只有664KB，而发送的千字节数更小，只有358KB

2)docker0和veth9f6bbcd的数据跟eth0基本一致，只是发送和接收相反，发送的数据较大，而接收的数据较小。这时Linux内部网桥转发导致的

3)异常地方：既然怀疑是网络接收中断的问题，还是重点来下eth0，接收的pps比较大，达到12607，而接收的BPS却很小，只有664KB。直观来看网络帧都是较小的，计算一下664*1024/12607=54字节，说明平均每个网络帧只有54字节，这显然是很小的网络帧，也就是我们通常所说的小包问题

那么有没有办法知道这是个什么杨的网络帧，以及从哪里发送过来的呢？
使用tcpdump抓取eth0上的包就可以了。
# -i eth0 只抓取 eth0 网卡，-n 不解析协议名和主机名
# tcp port 80 表示只抓取 tcp 协议并且端口号为 80 的网络帧
$ tcpdump -i eth0 -n tcp port 80
15:11:32.678966 IP 192.168.0.2.18238 > 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0
...

从结果看：192.168.0.2.18238 > 192.168.0.30.80 表示网络帧从192.168.0.2这个地址发送过到192.168.0.30的80端口，也就是从运行hping3机器的18238端口发送网络帧，目的为nginx所在机器的80端口

Flags[S]则表示这时一个SYN包

再加上前面用sar发现的，PPS超过12000的现象，现在我们可以确认，这就是从192.168.0.2这个地址发送过来的SYN FLOOD攻击

到这里，我们已经做了全套的性能诊断和分析。从系统的软中断使用率高这个现象出发，通过观察/proc/softirqs文件的变化情况，判断出软中断类型是网络接收中断，再通过sar和tcpdump 确认这时一个SYN FLOOD问题


SYN FLOOD问题最简单的解决方法，就是从交换机或者硬件防火墙中封掉来源IP，这样SYN FLOOD网络帧就不会发送到服务器中



小结：
软中断cpu使用率(softirq)升高是一种很常见的性能问题。虽然软中断的类型很多，但实际生产中，我们遇到的性能瓶颈大多是网络收发类型的软中断，特别是网络接收的软中断；；碰到这类问题时，可以借助sar、tcpdump等工具，做进一步分析



		 