iowait升高；也是最常见的一个服务器性能问题：

进程状态：
当iostat升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。从ps或者top命令的输出中，你可以发现他们处于D状态，也就是不可中断（Uninterruptible Sleep）。

top和ps是最常用的查看进程状态的工具。
$ top
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
28961 root      20   0   43816   3148   4040 R   3.2  0.0   0:00.01 top
  620 root      20   0   37280  33676    908 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:37.64 systemd
 1896 root      20   0       0      0      0 Z   0.0  0.0   0:00.00 devapp
    2 root      20   0       0      0      0 S   0.0  0.0   0:00.10 kthreadd
    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H
    6 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 mm_percpu_wq
    7 root      20   0       0      0      0 S   0.0  0.0   0:06.37 ksoftirqd/0

	
R：是Running或Runnable的缩写，表示进程再cpu得就绪队列中，正在运行或者正在等待运行

D：是Disk Sleep得缩写，也就是不可中断状态睡眠（Uniterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断

Z：是Zombie得缩写，表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它得资源（比如进程得描述符、pid等）

S：是Interruptible Sleep的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入R状态

I：是idle的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用D表示，但是对某些内核线程来说，它们有可能实际上并美哦与任何负载，用Idle正是为了区分这种情况。要注意，D状态的进程会导致平均负载升高，I状态的进程却不会

T: 或者t。也就是stopped或者Traced的缩写，表示进程处于暂停或者跟踪状态

X: 是Dead的缩写，表示进程已经消亡，所以你不会在top或者ps命令中看到它


不可中断状态，这其实是为了保证进程数据与硬件状态一致，并且正常情况下，不可中断状态在很短时间内就会结束，所以，短时的不可中断状态进程，一般可以忽略

但是如果系统或者硬件发生了故障，进程可能会在不可中断状态保持很久，甚至导致系统中出现大量不可中断进程。这时，你得注意下，系统是不是出现了I/O等性能问题


再看僵尸进程，这是多进程应用很容易碰到得问题。正常情况下，当一个进程创建了子进程后，它应该通过系统调用wait()或者waitpid()等待子进程结束，回收子进程得资源；而子进程在结束时，会向它得父进程发送SIGCHLD信号，所以，父进程还可以注册SIGCHLD信号的处理函数，异步回收资源

如果父进程没有这么做，或者是子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程

通常，僵尸进程持续的时间都比较短，在父进程回收它的资源后就会消失；或者在父进程退出后，由init进程回收后也会消亡

一旦父进程没有处理子进程的终止，还一直保持运行状态，那么子进程就会一直处于僵尸状态。大量的僵尸进程就会用尽pid进程号，导致新进程不能创建，所以这种情侣一定要避免


案例：
首先执行下面的命令运行案例应用：
$ docker run --privileged --name=app -itd feisky/app:iowait

然后，输入ps命令，确认案例应用已正常启动，如果一切正常，应该可以看到如下：
$ ps aux | grep /app
root      4009  0.0  0.0   4376  1008 pts/0    Ss+  05:51   0:00 /app
root      4287  0.6  0.4  37280 33660 pts/0    D+   05:54   0:00 /app
root      4288  0.6  0.4  37280 33668 pts/0    D+   05:54   0:00 /app


从这个界面，我们可以发现多个app进程已经启动，并且状态分别是Ss+和D+。其中：
S: 表示可中断睡眠状态
D: 表示不可中断睡眠状态 而+ 表示前台进程租

使用top查看：
# 按下数字 1 切换到所有 CPU 的使用情况，观察一会儿按 Ctrl+C 结束
$ top
top - 05:56:23 up 17 days, 16:45,  2 users,  load average: 2.00, 1.68, 1.39
Tasks: 247 total,   1 running,  79 sleeping,   0 stopped, 115 zombie
%Cpu0  :  0.0 us,  0.7 sy,  0.0 ni, 38.9 id, 60.5 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  0.7 sy,  0.0 ni,  4.7 id, 94.6 wa,  0.0 hi,  0.0 si,  0.0 st
...

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:38.59 systemd
...


第一行表示平均负载  1分钟   5分钟    15分钟

第二行的Tasks，有1个正在运行的进程，但僵尸进程比较多，而且还在不停的增加，说明有子进程在退出时没被清理

接下来看两个CPU的使用率情况，用户cpu和系统cpu都不高，但是iowait分别时60.5%和94.6%，有点不正常

最后再查看每个进程的情况，cpu使用率最高的进程只有0.3%，并不高；但是有两个进程处于D状态，它们有可能再等待I/O，但光凭这里并不能确定它们导致了iowait升高




1) iowait升高，导致系统平均负载升高，并且已经达到了系统cpu的个数
2) 僵尸进程再不断增多，看起来是应用程序没有正确清理子进程的资源

真相：
打开一个终端，执行如下：
# 先删除上次启动的案例
$ docker rm -f app
# 重新运行案例
$ docker run --privileged --name=app -itd feisky/app:iowait


iowait升高，首先会想到查询系统I/O情况
例如使用dstat来管擦cpu和I/O情况：
# 间隔 1 秒输出 10 组数据
$ dstat 1 10
You did not select any stats, using -cdngy by default.
--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--
usr sys idl wai stl| read  writ| recv  send|  in   out | int   csw
  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885
  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135
  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177
  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144
  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147
  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134
  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131
  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168
  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134

可以看出：每当iowait升高时（wait）；磁盘的读请求（read）都会很大。这说明iowait的升高跟磁盘的读请求有关，很可能就是磁盘读导致的；；但是到底时哪个进程在读磁盘呢？ 怀疑是前面分析的哪个D状态的僵尸进程
使用top 观察D状态的进程：
# 观察一会儿按 Ctrl+C 结束
$ top
...
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4340 root      20   0   44676   4048   3432 R   0.3  0.0   0:00.05 top
 4345 root      20   0   37280  33624    860 D   0.3  0.0   0:00.01 app
 4344 root      20   0   37280  33624    860 D   0.3  0.4   0:00.01 app
...

从top可以查看到D状态进程的pid，分别是4344和4345

此时使用pidstat来分析这两个进程：
# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据
$ pidstat -d -p 4344 1 3
06:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:38:51        0      4344      0.00      0.00      0.00       0  app
06:38:52        0      4344      0.00      0.00      0.00       0  app
06:38:53        0      4344      0.00      0.00      0.00       0  app

在这个输出中，KB_rd表示每秒读的KB数，KB_wr表示每秒写的KB数，iodelay表示I/O延迟。它们都是0，说明此时没有任何的读写，说明问题不是4344进程导致的
使用同样的办法分析4345，同样发现没有任何的磁盘读写


但是到底是哪个进程在进行磁盘读写呢？
# 间隔 1 秒输出多组数据 (这里是 20 组)
$ pidstat -d 1 20
...
06:48:46      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:47        0      4615      0.00      0.00      0.00       1  kworker/u4:1
06:48:47        0      6080  32768.00      0.00      0.00     170  app
06:48:47        0      6081  32768.00      0.00      0.00     184  app

06:48:47      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:48        0      6080      0.00      0.00      0.00     110  app

06:48:48      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:49        0      6081      0.00      0.00      0.00     191  app

06:48:49      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command

06:48:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:51        0      6082  32768.00      0.00      0.00       0  app
06:48:51        0      6083  32768.00      0.00      0.00       0  app

06:48:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:52        0      6082  32768.00      0.00      0.00     184  app
06:48:52        0      6083  32768.00      0.00      0.00     175  app

06:48:52      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command
06:48:53        0      6083      0.00      0.00      0.00     105  app
...

观察一会可以发现，的确是app进程在进行磁盘读，并且每秒读的数据有32MB，看来就是app问题。不过，app进程到底在执行什么I/O操作呢？

这里就需要回顾下进程用户态和内核态的却别。进程想要访问磁盘，就必须使用系统调用，所以重点就是找出app进程的系统调用：
strace是常用的跟踪进程系统调用的工具：
$ strace -p 6082
strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted

这里出现了一个奇怪的错误，strace命令居然失败了，并且命令报出的错误是没有权限。按理来说，我们所有的操作都已经是root用户运行了，为什么还会没有权限呢？怎么处理？

一般遇到这种问题时，首先查看下进程的状态是否正常：
$ ps aux | grep 6082
root      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] <defunct>

果然，进程6082已经变成了Z状态；也就是僵尸进程。僵尸进程都是已经退出的进程，所以，就没有办法继续分析它的系统调用

现在；系统iowait的问题还在继续；但是top、pidstat这类工具已经不能给出更多的信息了。这时我们就应该求助哪些基于事件记录动态追踪工具了
例如perf top   或者perf record -g  perf report
$ perf record -g
$ perf report

输出里的swapper是内核中的调度进程，可以先忽略掉
查看app得：app的确在通过系统调用sys_read()读取数据。并且从new_sync_read和blkdev_direct_IO能看出，进程正在对磁盘进行直接读，也就是绕过了系统缓存，每个读请求都会从磁盘直接读，这就可以解释我们观察到的iowait升高了

看来，罪魁祸首是app内部进行了磁盘的直接I/O;;
下面的问题就比较容易解决了，我们接下来应该从代码层面分析，究竟是哪里出现了直接读请求。
直接读写磁盘，对I/O敏感型应用（比如数据库系统）是很友好的，因为你可以在应用中直接控制磁盘的读写，但在大部分情况下，我们最好还是通过系统缓存来优化磁盘I/O



至于上面遗留问题中的僵尸进程：
既然僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，要解决掉他们，就要找到他们的根儿，也就是找出父进程，然后在父进程里解决

# -a 表示输出命令行选项
# p 表 PID
# s 表示指定进程的父进程
$ pstree -aps 3084
systemd,1
  └─dockerd,15006 -H fd://
      └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3991 -namespace moby -workdir...
              └─app,4009
                  └─(app,3084)

				  
可以看出:3084号进程的父进程是4009也就是app应用
所以我们接着查看app应用程序的代码。看看子进程结束的处理是否正确，比如有没有调用wait()或者waitpid(),或者，有没有注册SIGCHLD信号的处理函数
查看源码得：
int status = 0;
  for (;;) {
    for (int i = 0; i < 2; i++) {
      if(fork()== 0) {
        sub_process();
      }
    }
    sleep(5);
  }

  while(wait(&status)>0);

可以看出，循环出了问题；这段代码看起来调用了wait()函数等待子进程结束，但却错误的把wait()放到了for死循环的外面，也就说说wait()函数实际上并没有被调用到，我们把它移动至for循环里面就可以了

更新应用并重新启动后发现问题解决：



小结：
上述是一个多进程的案例，分析系统等待I/O的cpu使用率（也就是iowait%）升高的情况
虽然这个案例是磁盘I/O导致了iowait升高，不过，iowait高不一定代表I/O有性能瓶颈。当系统中只有I/O类型的进程在运行时，iowait也会很高，但实际上磁盘的读写远没有达到性能瓶颈的程度

因此，磁盘iowait升高时，需要先用dstat、pidstat等工具，确认是不是磁盘I/O的问题，然后再找是哪些进程导致了I/O 

等待I/O的进程一般是不可中断状态的，所以用ps或者top命令找到的D状态（即不可中断状态）的进程，多为可疑进程。上述案例中，在I/O操作后，进程又变成了僵尸进程，所以不能用strace直接分析这个进程的系统调用

这种情况下，我们用了perf工具，来分析系统的cpu时钟事件，最终稿发现是直接I/O导致的问题，这时，再检查源码中对应位置的问题，就很轻松了

而僵尸进程的问题相对容易排查，使用pstree找出父进程后，去查看父进程的代码，检查wait()或者waitpid()的调用，或是SIGCHLD信号处理函数的注册就行了


IO排查：  io使用很高也可使用该排查？？iostat -x 1\
top 查看iowait很高
pidstat -d -p $pid  1 3  发现没有Io产生
pidstat -d 1 20 管擦一会发现产生io进程 以及其pid
strace -p $pid 追踪程序  或者使用：perf record -g    perf report
找到系统调用io的函数；例如read  write  wait
修改应用代码


僵尸进程排查：
ps aux或者top 找到状态值为Z的pid
pstree |grep $pid   找到其父进程
然后查看父进程的源码中对子进程的结束处理；检查wait()或者waitpid()的调用，或是SIGCHLD信号处理函数的注册就行了






















