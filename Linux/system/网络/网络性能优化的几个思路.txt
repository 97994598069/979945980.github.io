1. 确定优化目标
跟CPU和I/O方面的性能优化一样，优化前，先问问自己，网络性能优化的目标是什么，换句话说，我们观察到的网络性能指标要达到多少才合适

实际上，虽然网络性能优化的整体目标，是降低网络延迟(如RTT)和提高吞吐量(如BPS和PPS)，但具体到不同应用中，每个指标的优化标准可能会不同，优先级顺序也大相径庭

就拿nat网关来说，由于其直接影响整个数据中心的网络出入性能，所以nat网关通常需要达到或接近线性转发，也就是说，PPS是最主要的性能目标
再如，对于数据库、缓存等系统，快速完成网络收发，即低延迟，是主要的性能目标
而对于我们经常访问的web服务来说，则需要同时兼顾吞吐量和延迟

所以，为了更客观合理地评估优化效果，我们应该首先明确优化的标准，即要对系统和应用程序进行基准测试,得到网络协议栈各层的性能


在基准测试时，可以按照协议栈的每一层来测试。由于底层是其上方各层的基础，底层性能也就决定了高层性能。所以，底层性能指标，其实就是对应高层的极限性能
我们从下往上来理解:
1)首先是网络接口层和网络层，它们主要负责网络包的封装、寻址、路由、以及发送和接收。每秒可处理的网络包数PPS，就是它们最重要的性能指标(特别是在小包的情况下)。可以用内核自带的发包工具pktgen来测试PPS的性能

2)再向上到传输层TCP和UDP，它们主要负责网络传输，对它们而言，吞吐量(BPS)、链接数以及延迟，就是最重要的性能指标，可以用iperf或netperf，来测试传输层的性能
不过要注意，网络包的大小，会直接影响这些指标的值，所以，通常，你需要测试一系列不同大小网络包的性能

3)再往上就到了应用层，最需要关注的是吞吐量(BPS)、每秒请求数以及延迟等指标。可以使用awk、ab等工具，来测试应用程序的性能



网络性能优化:
总的来说，先要获得网络基准测试报告，然后通过相关性能工具，定位出网络性能瓶颈，我们可以从应用程序、套接字、传输层、网络层以及链路层等几个角度，分别来查看网络性能优化的基本思路:
一. 应用程序
应用程序，通常通过套接字接口进行网络操作。由于网络收发通常比较耗时，所以应用程序的优化，主要就是对网络I/O和进程自身的工作模型的优化
从网络I/O的角度来说，主要有下面两种优化思路:
1) 最常用的I/O多路复用技术epoll，主要用来取代select和poll，这其实是解决C10K问题的关键，也是目前很多网络应用默认使用的机制
2) 使用异步I/O(Asynchronous I/O,AIO).AIO允许应用程序同时发起很多I/O操作，而不用等待这些操作完成。等到I/O完成后，系统会用事件通知的方式，告诉应用程序结果。不过AIO的使用比较复杂，需要小心处理很多边缘情况
而从进程的工作模型来说，也有两种不同的模型用来的优化
1_1) 主进程+多个worker子进程，其中，主进程负责管理网络连接，而子进程负责实际的业务处理，这也是最常用的一种模型
1_2) 监听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且开启SO_REUSEPOST选项，由内核负责，把请求负载均衡到这些监听进程中去

除了网络I/O和进程的工作模型外，应用层的网络协议优化，也是至关重要的一点，如下:
1.使用长连接取代短连接，可以显著降低TCP建立连接的成本。在每秒请求次数较多时，这样做的效果非常明显
2.使用内存等方式，来缓存不常变化的数据，可以降低网络I/O次数，同时加快应用程序的响应速度
3.使用protocol buffer等序列化的方式，压缩网络I/O的数据量，可以提高应用程序的吞吐
4.使用DNS缓存、预取、HTTPDNS等方式，减少DNS解析的延迟，也可以提升网络I/O的整体速度


二.套接字
套接字可以屏蔽掉Linux内核中不同协议的差异，为应用程序提供统一的访问接口，每个套接字，都有一个读写缓冲区
1) 读缓冲区，缓存了远端发过来的数据，如果读缓冲区已满，就不能再接收新的数据
2) 写缓冲区，缓存了要发出去的数据，如果写缓冲区已满，应用程序的写操作就会被阻塞
所以，为了提高网络的吞吐量，通常需要调整这些缓冲区的大小，比如:
1) 增大每个套接字的缓冲区大小net.core.optmem_max
2) 增大套接字接收缓冲区大小net.core.rmem_max和发送缓冲区大小net.core.wmem_max 
3) 增大TCP接收缓冲区大小net.ipv4.tcp_rmem和反缓冲区大小net.ipv4.tcp_wmem 
至于套接字的内核选项，可参考如下表格:
	
套接字优化办法	                    内核选项	         参考设置
增大每个套接字的缓冲区大小	    net.core.optmem_max	       81920
增大套接字接收缓冲区大小	    net.core.rmem_max	       513920
增大套接字发送缓冲区大小	    net.core.wmem_max	       513920
增大TCP接收缓冲区范围	        net.ipv4.tcp_rmem	       4096    87380  16777216
增大TCP发送缓冲区范围	        net.ipv4.tcp_wmem	       4096    65536  16777216
增大UDP缓冲区范围	            net.ipv4.udp_mem	       188562  251418  377124

不过需要注意几点
tcp_rmem和tcp_wmem的三个数值分别是min，default，max，系统会根据这些设置，自动调整TCP接收/发送缓冲区的大小
udp_mem的三个数值分别是min，pressure，max，系统会根据这些设置，自动调整udp发送缓冲区的大小
表格中的数值只提供参考价值，具体应该设置成多少，还需要根据实际的网络状况来去确定，比如，发送缓冲区大小，理想数值是吞吐量*延迟，这样才可以达到最大网络利用率


除此之外，套接字接口还提供了一些配置选项，用来修改网络连接的行为:
1.为TCP连接设置TCP_NODELAY后，就可以禁用Nagle算法
2.为TCP连接开启TCP_CORK后，可以让小包聚合成大包再发送(注意会阻塞小包的发送)
3.使用SO_SNDBUF和SO_RCVBUF,可以分别调整套接字发送缓冲区和接收缓冲区的大小


在优化网络性能时，可以结合Linux系统的网络协议栈和网络收发流程，然后从应用程序、套接字、传输层、网络层再到链路层等每个层次，进行逐层优化,比如应用程序和套接字的优化思路:
1) 应用程序中，主要优化I/O模型、工作模型以及应用层的网络协议
2) 套接字层中，主要优化套接字的缓冲区大小



怎么从传输层、网络层以及链路层中优化Linux网络性能:

一、传输层的优化  主要包含tcp和udp的优化:
1.传输层最重要的是TCP和UDP协议，所以这儿的优化，其实主要就是对这两种协议的优化:
 TCP的优化
1_1. 在请求数比较大的场景下，可以会看到大量的TIME_WAIT状态的链接，它们会占用大量内存和端口资源。这时，可以优化与TIME_WAIT状态相关的内核选项，如下:
1) 增大处于TIME_WAIT状态的链接数量net.ipv4.tcp_max_tw_buckets,并增大连接跟踪表的大小net.netfilter.nf_conntrack_max
2) 减小net.ipv4.tcp_fin_timeout和net.netfilter.nf_conntrack_tcp_timeout_time_wait,让系统尽快释放它们所占用的资源
3) 开启端口复用net.ipv4.tcp_tw_reuse.这样，被TIME_WAIT状态占用的端口，还能用到新建的链接中
4) 增大本地端口的范围net.ipv4.ip_local_port_range。这样就可以支持更多连接，提高整体的并发能力
5) 增加最大文件描述符的数量，可以使用fs.nr_open,设置系统的最大文件描述符数；或在应用程序的systemd配置文件中，配置LimitNOFILE,设置应用程序的最大文件描述数

1_2. 为了缓解SYN FLOOD等，利用TCP协议特点进行攻击而引发的性能问题，可以考虑优化与SYN状态相关的内核选项，如下:
1) 增大TCP半连接的最大数量net.ipv4.tcp_max_syn_backlog, 或者开启TCP SYN Cookies net.ipv4.tcp_syncookies,来绕开半连接数量限制的问题(这两个选项不能同时使用)
2) 减少SYN_RECV状态的链接重传SYN+ACK包的次数net.ipv4.tcp_synack_retries

1_3. 在长连接的场景中，通常使用keepalive来检测TCP状态，以便对端连接断开后，可以自动回收。但是，系统默认的keepalive探测间隔和重试次数，一般都无法满足应用程序的性能要求。所以，这时候你需要优化与keepalive相关的内核选项，如下:
1) 缩短最后一次数据包到keepalive探测包的间隔时间net.ipv4.tcp_keepalive_time
2) 缩短发送keepalive探测包的间隔时间net.ipv4.tcp_keepalive_intvl 
3) 减少keepalive探测失败后，一直到通知应用程序前的重试次数net.ipv4.tcp_keepalive_probes 

TCP优化办法	内核选项	参考设置
增大处于TIME_WAIT状态的连接数量	                        net.ipv4.tcp_max_tw_buckets	                                       1048576
增大连接跟踪表的大小	                                net.netfliter.nf_conntrack_max	                                   1048576
缩短处于TIME_WAIT状态的超时时间	                        net.ipv4.tcp_fin_timeout	                                       15
缩短连接跟踪表中处于TIME_WAIT状态连接的超时时间	        net.netfliter.nf_conntrack_tcp_timeout_time_wait	               30
允许TIME_WAIT状态占用的端口还可以用到新建的链接中	    net.ipv4.tcp_tw_reuse	                                           1
增大本地端口号的范围	                                net.ipv4.ip_local_port_range	                                   10000 65000
增加系统和应用程序的最大文件描述符数	                fs.nr_open(系统)，systemd配置文件中的LimitNOFILE(应用程序)	       1048576
增加半连接的最大数量	                                net.ipv4.tcp_max_syn_backlog	                                   16384
开启SYN Cookies	                                        net.ipv4.tcp_syncookies	                                            1
缩短发送keepalive探测包的间隔时间	                    net.ipv4.tcp_keepalive_intvl	                                    30
减少keepalive探测失败后通知应用程序前的重试次数	        net.ipv4.tcp_keepalive_probes	                                    3
缩短最后一次数据包到keepalive探测包的间隔时间	        net.ipv4.tcp_keepalive_time	                                        600

优化TCP性能时，你还要注意，如果同时使用不同的优化方法，可能会产生冲突
比如，就像网络请求延迟的案例中服务器端开启Nagle算法，而客户端开启延迟确认机制，就很容器导致网络延迟增大
另外，在使用ant的服务器上，如果开启net.ipv4.tcp_tw_resycle，就很容易导致各种连接失败，实际上，由于坑太多，这个选项已经在内核4.1中删除了


UDP的优化:
UDP提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障，所以，UDP优化，相对于TCP来说，要简单得多。优化如下:
1) 跟套接字部分提到的一样，增大套接字缓冲区大小以及UDP缓冲区范围
2) 跟前面TCP部分提到的一样，增大本地端口号的范围
3) 根据MTU大小，调整UDP数据包的大小，减小或者避免分片的发生





二、网络层的优化
网络层，负责网络包的封装、寻址和路由，包括IP、ICMP等常见协议，在网络层，最主要的优化其实就是对路由、IP分片以及ICMP等进行调优
1.从路由和转发的角度出发，可以调整如下内核选项:
1) 在需要转发的服务器中，比如用作nat网关的服务器或者使用Docker容器时，开启IP转发，即设置net.ipv4.ip_forward=1
2) 调整数据包的生存周期TTL，比如设置net.ipv4.ip_default_ttl=64。注意，增大该值会降低系统性能
3) 开启数据包的反向地址校验，比如设置net.ipv4.conf.eth0.rp_filter=1这样可以防止IP欺骗，并减少伪造IP带来的DDoS问题

2.从分片的角度出发，最主要的是调整MTU(Maximum Transmission Unit)的大小
1) 通常，MTU的大小应该根据以太网的标准来设置，以太网标准规定，一个网络帧最大为15188.那么丢掉以太网头部的18B后，剩余1500就是以太网MTU的大小
2) 在使用VXLAN GRE等叠加网络技术，要注意，网络叠加会使原来的网络包变大，导致MTU也需要调整
比如，就以VXLAN为例，它在原来报文的基础上，增加了14B的以太网头部、8B的VXLAN头部、8BUDP头部以及20B的IP头部，换句话说，每个包比原来增大了50B；所以我们就需要把交换机、路由器等的MTU增大到1550，或者把VXLAN封包前(比如虚拟化环境中的虚拟网卡)的MTU减小为1450;另外，现在很多网络设备支持巨帧，如果是这种环境，你还可以把MTU调大为9000，以提高网络吞吐量

3.从ICMP的角度出发，为了避免ICMP主机探测，ICMP Flood等各种网络问题，可以通过内核选项来限制ICMP的行为:
1) 可以禁止ICMP协议，即设置net.ipv4.icmp_echo_ignore_all=1.这样，外部主机就无法通过ICMP来探测主机
2) 或者，还可以禁止广播ICMP，即设置net.ipv4.icmp_echo_ignore_broadcasts=1



三、链路层的优化
网络层下面就是链路层，链路层负责网络包在网络中的传输，比如MAC寻址、错误侦测以及通过网卡传输网络帧等。自然，链路层的优化，也是围绕这些基本功能进行的，优化如下:
1_1.链路层负责网络包在物理网络中传输，比如mac寻址、错误帧测以及通过网卡传输网络帧等，自然，链路层优化，也是围绕这些基本功能进行的，如下:
1)比如，可以为网卡硬中断配置CPU亲和性(smp_affinity),或者开启irqbalance服务
2)再如，可以开启RPS(Receive Packet Steering)和RFS(Receive Flow steering),将应用程序和软中断的处理，调度到相同CPU上，这样就可以增加CPU缓存命中率，减少网络延迟
另外，现在的网卡都有很丰富的功能，原来在内核中通过软件处理的功能，可以卸载到网卡中，通过硬件来执行
3) TSO(TCP segmentation Offload)和UFO(UDP Fragmentation Offload);在TCP/UDP协议中直接发送大包；而TCP包的分段(按照MSS分段)和UDP的分片(按照MTU分片)功能，由网卡来完成
4) GSO(Generic Segmentation Offload):在网卡不支持TSO/UFO时，将TCP/UDP包的分段，延迟到进入网卡前再执行，这样，不仅可以减少CPU的消耗，还可以再发生丢包时只重传分段后的包
5) ....


1_2. 对于网络接口本身，也有很多方法，可以优化网络吞吐量
1)比如可以开启网络接口的多对列功能，这样每个队列就可以用不同的中断号，调度到不同的CPU上执行，从而提升网络的吞吐量
2)再如，你可以增大网络接口的缓冲区大小，以及队列长度等，提升网络传输的吞吐量(注意，这可能导致延迟增大)
3)还可以使用Traffic Control工具，为不同网络流量配置QoS




我们分析和定位网络瓶颈，也是基于这些网络层进行的，而定位出网络性能瓶颈后，我们就可以根据瓶颈所在的协议层，进行优化:
1) 在应用程序中，主要优化I/O模型、工作模型以及应用层的网络协议
2) 在套接字层中，主要优化套接字的缓冲区大小
3) 在传输层中，主要优化TCP和UDP协议
4) 在网络层中，主要优化路由、转发、分片以及ICMP协议
5) 最后，在链路层中，主要是优化网络包的收发、网络功能卸载以及网卡选项


































