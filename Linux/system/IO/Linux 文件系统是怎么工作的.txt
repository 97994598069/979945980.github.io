1.磁盘为系统提供了最基本的持久化存储
2.文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构
那么,磁盘和文件系统是怎么工作的呢？又有哪些指标可以衡量它们的性能呢？

linux文件系统为每个文件都分配两个数据结构，索引节点和目录项。它们主要用来记录文件的元信息和目录结构
1)索引节点，简称inode，用来记录文件的元数据，比如inode编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化储存到磁盘中。所以记住，索引节点同样占用磁盘空间
2)目录项，简称为dentry，用来记录文件的名字、索引节点指针以及其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存

索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，可以理解为一个文件可以有多个别名

举例：
通过硬连接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是连接同一个文件，所以，它们的索引节点相同

索引节点和目录项纪录了文件的元数据，以及文件间的目录关系，那么具体来说，文件数据到底是怎么储存的呢？

实际上，磁盘读写的最小单位是扇区，然而扇区只有512B大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元来管理数据。常见的逻辑块大小为4KB，也就是由联系的8个扇区组成


需注意:
1) 目录项本身就是一个内存缓存，而索引节点则是储存在磁盘中的数据。在前面的Buffer和cache原理中提过，为了协调慢速磁盘与快速cpu的性能差异，文件内容会缓存到页缓存cache中；那么这些索引节点自然也会缓存到内存中，加速文件的访问

2) 磁盘在执行文件系统格式化时，会被分成三个储存区域，超级块、索引节点区和数据块区，其中；1.超级块，储存整个文件系统的状态。2.索引节点区，用来储存索引节点  3.数据块区，则用来储存文件数据





虚拟文件系统
目录项、索引节点、逻辑块以及超级块，构成了Linux文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统VFS（Virtual File System）

VFS定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟VFS提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节


文件系统可以分为三类：
1.基于磁盘的文件系统，也就是把数据直接储存在计算机本地挂载的磁盘中。常见的ext4 xfs overlayfs
2.基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配储存空间，但会占用内存。我们经常用到的/proc文件系统，其实就是一种最常见的虚拟文件系统，此外/sys文件系统也属于这一类，主要向用户空间导出层次化的内核对象
3.网络文件系统，也就是用来访问其他计算机数据文件系统，如果NFS，SMB，iSCSI等

上述的这些文件系统，要先挂载到VFS目录树种的某个子目录（称为挂载点），然后才能访问其中的文件。例如基于磁盘的文件系统，在安装系统时，要先挂载一个根目录（/）,在根目录下再把其他文件系统(比如其他的磁盘分区、/proc文件系统、/sys文件系统、NFS等)挂载进来



文件系统I/O
把文件系统挂载到挂载点后，就能通过挂载点，再去访问它的管理文件。VFS提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用


文件读写方式的各种差异，导致I/O的分类多种多样。最常见的有，缓冲与非缓冲I/O、直接与非直接IO、阻塞与非阻塞I/O、同步与异步I/O等
1.根据是否利用好标准库缓存，可以把文件I/O分为缓冲I/O与非缓冲I/O 
  1)缓冲I/O,是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件
  2)非缓冲I/O,是指直接通过系统调用来访问文件，不再经过标准库缓存
备注：
1)这里所说的"缓冲"，是指标准库内部实现的缓存，比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来
2)无论缓冲I/O还是非缓冲I/O，它们最终还是要经过系统调用来访问文件，系统调用后还会通过页缓存，来减少磁盘的I/O操作

2.根据是否利用操作系统的页缓存，可以把文件I/O分为直接I/O与非直接I/O 
 1)直接I/O,是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件
 2)非直接I/O正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘
备注:
想要实现直接I/O,需要你在系统调用中，指定O_DIRECT标志。如果没有设置过，默认的是非直接I/O 
不过要注意，直接I/O、非直接I/O,本质上还是和文件系统交互。如果是在数据库等场景中，还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸I/O 

3.根据应用程序是否阻塞自身运行，可以把文件I/O分为阻塞I/O和非阻塞I/O 
  1)所谓阻塞I/O，是指应用程序执行I/O操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务
  2)所谓非阻塞I/O,是指应用程序执行I/O操作后，不会阻塞当前的线程，可以继续执行其他任务，随后再通过轮询或者事件通知的形式，获取调用的结果
备注:
访问管道或者网络套接字时，设置O_NONBLOCK标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问

4.根据是否等待响应结果，可以把文件I/O分为通过和异步I/O 
  1)所谓同步I/O,是指应用程序执行I/O操作后，要一直等待整个I/O完成后，才能获得I/O响应
  2)所谓异步I/O，是指应用程序执行I/O操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次I/O完成后，响应会用事件通知的方式，告诉应用程序
  
例如：在操作文件时，如果设置了O_SYNC或者O_DSYNC标志，就代表同步I/O。如果设置了O_DSYNC，就要等文件数据写入磁盘后，才能放回；而O_SYNC,则是在O_DSYNC基础上，要求文件元数据也要写入磁盘后，才能返回


性能观测：
df -h  查看空间，有时虽然报空间不足的错误，但是df 查看后还有很多，此时可以查看下inode的使用量 df -i df -i /dev/sda1';;inode的个数一般由格式化工具自动生成的，当发现索引节点（inode）不足，但是磁盘空间充足时，很可能就是过多小文件导致的。一般来说删除这些小文件或者把它们移动到索引节点充足的其他磁盘中就可以解决这个问题

缓存：
可以使用free或者vmstat，来观察页缓存的大小。
free输出的是cache。是页缓存和可回收Slab缓存的和，可以从/proc/meminfo直接得到它们的大小
$ cat /proc/meminfo | grep -E "SReclaimable|Cached" 
Cached:           748316 kB 
SwapCached:            0 kB 
SReclaimable:     179508 kB 

实际上，内核使用slab机制，管理目录项和索引节点的缓存。/proc/meminfo只给出了slab的整体大小，具体到每一种slab缓存，还要查看/proc/slabinfo这个文件

查看所有目录项和各种文件系统索引节点的缓存情况：
$ cat /proc/slabinfo | grep -E '^#|dentry|inode' 
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail> 
xfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 
... 
ext4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 
sock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 
shmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 
proc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 
inode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 
dentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 

上述：
dentry行表示目录项缓存，inode_cache行表示VFS索引节点缓存，其余的则是各种文件系统的索引节点缓存
/proc/slabinfo列比较多，具体含义可以man slabinfo。  在实际性能分析中经常使用slabtop来找到占用内存最多的缓存类型
# 按下 c 按照缓存大小排序，按下 a 按照活跃对象数排序 
$ slabtop 
Active / Total Objects (% used)    : 277970 / 358914 (77.4%) 
Active / Total Slabs (% used)      : 12414 / 12414 (100.0%) 
Active / Total Caches (% used)     : 83 / 135 (61.5%) 
Active / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) 
Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K 

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME 
69804  23094   0%    0.19K   3324       21     13296K dentry 
16380  15854   0%    0.59K   1260       13     10080K inode_cache 
58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache 
   485    413   0%    5.69K     97        5      3104K task_struct 
  1472   1397   0%    2.00K     92       16      2944K kmalloc-2048 
从上述结果可以看到，在系统中，目录项和索引节点占用了最多的slab缓存，不过他们占用的内存其实并不大，加起来也就是23MB左右
















