deployment实际上并不足以覆盖所有的应用编排问题:
造成这个问题的根本原因，在于Deployment对应用做了一个简单化的假设：
它认为一个应用的所有pod，是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，deployment就可以通过pod模板创建的pod；不需要的时候，deployment就可以“杀掉”任意一个pod


但是在实际的场景中，并不是所有的应用都可以满足这样的要求。尤其是分布式应用，它的对个实例之间，往往有依赖关系，比如，主从关系，主备关系。
还有就是数据储存类应用，它的多个实例，往往都会在本地磁盘上保存一份数据，而这些实例一旦被杀掉，即便重建出来，实例与数据之间的对应关系也已经丢失，从而导致应用失败

所以这种实例之间不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）


容器技术诞生后，大家很快发现，它用来封装“无状态应用”（Stateless Application）,尤其是web服务，非常好用，但是，一旦你想要用容器运行“有状态应用”，其困难程度就会直线上升。而且，这个问题解决起来，单纯依靠容器技术本身已经无能为力，这也就导致了很长一段时间内，“有状态应用”几乎成了容器技术圈子的“忌讳”，

不过k8s成了第一个吃螃蟹的人，得益于“控制器模式”的设计思想，k8s项目很早就在Deployment的基础上，扩展出了对“有状态应用的”初步支持，这个就是statefulSet


StatefulSet的设计其实非常容易理解，它把真实世界里的应用状态，抽象为两种情况:
1.拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动，比如应用的主节点A要先于从节点B启动。而如果你把A和B两个Pod删掉，它们再次被创建出来时，也必须严格按照这个顺序才行。并且，新创建出来的pod，必须和原来pod的网络标识一样，这样原先的访问者，才能使用同样的方法，访问到这个新pod

2.储存状态。这种情况意味着，应用的多个实例分别绑定了不同的储存数据，对于这些应用实例来说，pod A第一次读取到的数据，和隔了十分钟再次读取到的数据，应该是同一份，哪怕在此期间pod A被重新创建过，这种情况最典型的例子，就是一个数据库应用的对个储存实例


所以，SatefulSet的核心功能，就是通过某种方式记录这些状态，然后在pod被重新创建时，能够为新pod恢复这些状态


在理解StatefulSet的工作原理之前，必须理解k8s项目非常实用的概念：Headless Service 
service是k8s项目中用来将一组pod暴露给外界访问的一种机制，比如，一个deployment有3个pod，那么我就可以定义一个Service，然后用户只要能访问到这个service，它就能访问某个具体的pod

那么，这个service又是如何被访问的呢？
第一种方式，是以Service的VIP（Virtual IP,即虚拟IP）方式。比如:当我访问10.0.23.1这个service的IP地址时，10.0.23.1其实就是一个VIP，它会把请求转发到该service所代理的某一个pod上。

第二种方式：就是以Service的DNS方式，比如，这个时候只要我访问“my-svc.my-namespace.svc.cluster.local”这条记录，就可以访问到名叫my-svc的service所代理的某一个pod

而在第二种方法 Service DSN的方式下，具体还可以分为两种处理方法:
1.是Normal Service 这种情况下，你访问my-svc.my-namespace.svc.cluster.local解析到的，正是my-svc这个service的VIP，后面的流程就跟VIP方式一致了

2.是Headless Service 这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是my-svc代理的某一个pod的Ip地址，可以看到，这里的区别在于Headless Service不需要分配一个VIP，而是可以直接以DNS记录的方式解析出来被代理pod的IP地址

那么这样的设计有什么用呢？
如下是一个标准的Headless Service对应的yaml文件:
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx

可以看到，所谓Headless Service，其实仍是一个标准Service的Yaml文件，只不过，它的clusterIP字段的值是:None,即：这个service，没有一个VIP作为“头”。这页就是Headles的含义，所以，这个service被创建后并不会被分配一个VIP，而是会以DNS记录的方式暴露出它所代理的Pod

然后关键：
当你按照这样的方式创建了一个Headless Service之后，它所代理的所有Pod的IP地址，都会被绑定一个这样格式的DNS记录，如下:
<pod-name>.<svc-name>.<namespace>.svc.cluster.local
这个DNS记录，正是k8s项目为pod分配的唯一的"可解析身份"（resolvable Identity）。有了这个“可解析身份”，只要你知道了一个pod的名字，以及它对应的service的名字，你就可以非常确定地通过这条DNS记录访问到Pod的IP地址。

那么，StatefukSet又是如何使用这个DNS记录来维持pod的拓扑状态的呢？
StatefulSet的YAML文件:
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web

这个YAML文件，和我们在前面用到的nginx-deployment的唯一却别，就是多了一个ServiceName=nginx字段

这个字段的作用，就是告诉StatefulSet控制器，在执行控制循环（Control Loop）的时候，请使用nginx这个Headless Service来保证pod的“可解析身份”

所以，当你通过kuebectl create创建了上面这个service和StatefukSet之后，就会看到如下两个对象：
$ kubectl create -f svc.yaml
$ kubectl get service nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         <none>        80/TCP    10s

$ kubectl create -f statefulset.yaml
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         1         19s

这时候，如果你手比较快的话，还可以通过kubectl的-w参数，即:Watch功能，实时查看StatefulSet创建两个有状态实例的过程：
备注:
如果手不够快的话，Pod很快就创建完了，不过，你依然可以通过这个StatefulSet的Events看到这些信息：
$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         20s


通过上面这个pod的创建过程，我们不难看到，StatefulSet给它所管理的所有pod的名字，进行了编号，编号规则是:-
而且这些编号都是从0开始累加，与StatefulSet的每一个pod实例一一对应，绝不重复

更重要的是，这些pod的创建，也是严格按照编号顺序进行的，比如，在web-0进入到Running状态、并且细分状态（conditions）成为Ready值卡你，web-1会一直处于pending状态
备注：
Ready状态再一次提醒了我们，为pod设置livenessProbe和readlinessProbe的重要性

当两个pod都进入了Running状态之后，就可以查看到它们各自唯一的“网络身份”了
我们使用kubectl exec 命令进入到容器中查看它们的hostname:
$ kubectl exec web-0 -- sh -c 'hostname'
web-0
$ kubectl exec web-1 -- sh -c 'hostname'
web-1


可以看到，这两个Pod的hostname与Pod名字是一致的，都被分配了对应的编号，接下来，我们再试着以DNS的方式，访问一下这个Headless Service：
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh 

通过这个命令，我们启动了一个一次性的pod，因为-rm意味着pod退出后就会被删除掉。然后，在这个pod的容器里面，我们尝试用nslookup命令，解析一下pod对应的Headless Service：
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh
$ nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

$ nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.7

从nslookup命令的输出结果中，我们可以看到，在访问web-0.nginx的时候，最后解析到的，正是web-0这个pod的IP地址:而当访问web-1.nginx的时候，解析到的则是web-1的IP地址

这时候，如狗你在另外一个Terminal里把这两个“有状态应用”的pod删掉：
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

然后，再在当前Terminal里watch一下这两个pod的状态变化，你就会发现一个有趣的现象:
$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         32s

可以看到，当我们把这两个pod删除之后，k8s会按照原先编号的顺序，创建出两个新的pod。并且k8s依然为它们分配了原来的相同的“网络身份”：web-0.nginx和web-1.nginx;通过这种严格的对应规则，StatefulSet就保证了Pod网络标识的稳定性


比如，如果web-0是一个需要先启动的主节点，web-1是一个后启动的从节点，那么只要这个StateFulSet不被删除，你访问web-0.nginx时始终都会落在主节点上，访问web-1.nginx时，则始终都会落在从节点上，这个关系绝对不会发生任何变化

所以，如狗我们再用nslookup命令，查看一下这个新pod对应的Headless.service的话：
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh 
$ nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.8

$ nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8

此时可以看到，在这个StatefulSet中，这两个新pod的“网络标识”（比如：web-0.nginx和web-1.nginx）,再次解析到了正确的Ip地址（比如：web-0 pod的IP地址10.244.1.8）

通过这种方法，k8s就成功地将pod的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照pod的“名字+编号”的方式固定了下来。此外，k8s还未每一个pod提供了一个固定并且唯一的访问入口，即：这个Pod对应的DNS记录

这些状态，在StatefulSet的整个生命周期里都会保持不变，绝不会因为对应的pod的删除或者重新创建而失效

不过，相信你也注意到了：尽管web-0.nginx这条记录本身不会变，但它解析到的pod的IP地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用DNS记录或者hostname的方式，而绝不应该直接访问这些pod的Ip地址






		  



























 



































