StatefulSet对存储状态的管理机制，这个机制，主要使用的是一个叫作Persistent volume claim的功能

在前面介绍pod的时候，我曾提到过要在一个pod里声明Volume，只要在pod里假上spec.volumes字段即可。然后，你就可以在这个字段里定义一个具体类型的volume了，比如：hostpath

可是，有没有这个一个场景：如果你并不知道有哪些Volume类型可以用，要怎么办呢？
更具体地说，作为一个应用开发者，我可能对持久化存储项目（比如ceph、GlusterFS等）一窍不通，也不知道公司的k8s集群到底是怎么搭建出来的，我也自然不会编写它们对应的volume定义文件

所谓"术业有专攻"，这些关于volume的管理和远程持久化储存的知识，不仅超越了开发者的知识储备，还会有暴露公司基础设施秘密的风险


比如，下面这个例子，就是一个声明了ceph RBD类型volume的pod:
apiVersion: v1
kind: Pod
metadata:
  name: rbd
spec:
  containers:
    - image: kubernetes/pause
      name: rbd-rw
      volumeMounts:
      - name: rbdpd
        mountPath: /mnt/rbd
  volumes:
    - name: rbdpd
      rbd:
        monitors:
        - '10.16.154.78:6789'
        - '10.16.154.82:6789'
        - '10.16.154.83:6789'
        pool: kube
        image: foo
        fsType: ext4
        readOnly: true
        user: admin
        keyring: /etc/ceph/keyring
        imageformat: "2"
        imagefeatures: "layering"
其一，如果不懂得ceph RBD得使用方法，那么这个pod里volumes字段，你十有八九也完全看不懂，其二，这个ceph RBD对应得储存服务器得地址、用户名、授权文件得位置，也都被轻易地暴露给了全公司得所有开发人员，这是一个典型得信息被“过度暴露”的例子

这也是为什么，在后来的演化中，k8s项目引入了一组叫做persistent volume Claim(PVC)和persistent volume（PV）的API对象，大大降低了用户声明和使用持久化volume的门槛

举个例子，有了PVC之后，一个开发人员想要使用一个volume,只需要简单的两步即可:
第一步：定义一个PVC，声明想要的volume的属性：
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

可以看到，在这个PVC对象里，不需要任何关于volume细节的字段，只有描述性的属性和定义。比如，storage：1Gi，表示我想要的volume大小至少是1GiB;accessModes: ReadWriteOnce，表示这个volume的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享


第二步，在应用的pod中，声明使用这个PVC:
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
    - name: pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim


可以看到，在这个pod的volume定义中我们只需要声明它的类型是persistentVolumeClaim，然后指定PVC的名字，而完全不必关心volume本身的定义

这时候，只要我们创建这个PVC对象，k8s就会自动为它绑定一个符合条件的volume,可是这些符合条件的volume又是从哪里来的呢？
答案是：它们来自于由运维人员维护的PV（persistent volume）对象。接下来，我们一起看一个常见的PV对象的yaml文件：
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  rbd:
    monitors:
    - '10.16.154.78:6789'
    - '10.16.154.82:6789'
    - '10.16.154.83:6789'
    pool: kube
    image: foo
    fsType: ext4
    readOnly: true
    user: admin
    keyring: /etc/ceph/keyring
    imageformat: "2"
    imagefeatures: "layering"

可以看到，这个PV对象的spec.rbd字段，正是我们前面介绍过的ceph RBD Volume的详细定义。而且，它还声明了这个PV的容量是10GiB.这样，k8s就会为我们刚刚创建的PVC对象绑定这个PV。

所以，k8s中PVC和PV的设计，实际上类似于“接口”和“实现”的思想，开发者，只要知道并会使用“接口”，即：PVC，而运维则负责给“接口”绑定具体的实现，即：PV


这种耦合就比秒了因为向着开发者暴露过多的储存系统细节而带来的隐患。此外，这种职责的分离，往往也意味着出现事故时可以更容易定位问题和明确责任，从而避免“扯皮”现象的出现。


而PVC PV的设计，也使得StatefulSet对储存状态的管理成为了可能
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi

		  
这次，我们为这个StatefulSet额外添加了一个VolumeClaimTemPlates字段，从名字就可以看出来，它跟Deployment里pod模板（PodTemplate）的作用类似。也就是说，凡是被这个StatefulSet管理Pod，都会声明一个对应的PVC；而这个PVC的定义，就来自于volumeClaimTemplates这个模板字段，更重要的是，这个PVC的名字，会被分配一个与这个pod完全一致的编号。


这个自动创建的PVC与PV绑定成功后，就会进入Bound状态，这就意味着这个POD可以挂载并使用这个PV了

如果还是不太理解PVC的话，可以先记住这样一个结论：PVC其实就是一种特殊的volume.只不过一个PVC具体是什么类型的volume，要在跟某个PV绑定之后才知道

当然，PVC和PV的绑定得以实现得前提是运维人员已经在系统里创建号了符合条件得PV（比如，前面提到得pv-volume）;或者你得k8s集群运行在公有云上，这样，k8s就会通过DynaMic Provisisoning得方式，自动为你创建与PVC匹配得PV


所以，我们在使用kubectl create创建了StatefulSet之后，就会看到k8s集群里出现了两个PVC：
$ kubectl create -f statefulset.yaml
$ kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s


可以看到，这些PVC，都以“<PVC名字>--<StatefulSet名字>-<编号>”的方式命名，并且处于Bound状态

StatefulSet创建出来的所有Pod，都会声明使用编号的PVC，比如，在名叫web-0的pod的volumes字段，它会声明使用名叫www-web-0的PVC，从而挂载到这个PVC所绑定的PV

所以，我们可以使用如下命令，在pod的volume目录里写入一个文件，来验证一下上述volume的分配情况：
$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done

如上，通过kubectl exec指令，我们在每个pod的volume目录里，写入了一个index.html文件。这个文件的内容，正是pod的hostname比如，我们在web-0的index.html里写入的内容就是“hello web-0”

此时，如果你在这个pod容器里访问http://localhost，你可以实际访问到的就是pod里nginx服务器进程，而它会为你返回/usr/share/nginx/html/index.html里的内容，这个操作如下：
$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
hello web-0
hello web-1


现在关键来了：
如果你使用kubectl delete命令删除这两个pod，这些volume里的文件会不会丢失南北？
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted


可以看到，正如我我们前面所将，在被删除之后，这两个pod会被按照编号顺序被重新创建出来，而这时候，如果你在新创建的容器里通过访问http：//localhost的方式去访问web-0里的nginx服务：
# 在被重新创建出来的 Pod 容器里访问 http://localhost
$ kubectl exec -it web-0 -- curl localhost
hello web-0

就会发现，依旧会返回hello web-0,也就是说，原先与名叫web-0的pod绑定的PV，在这个Pod被重新创建之后，依然同新的名字叫web-0的pod绑定在了一起，对于pod-web-1来说，也是完全一样的情况


怎么做到的呢？
其实分析一下StatefukSet控制器恢复这个pod的过程，你就可以很容器理解了。

首先，当你把一个pod，比如web-0,删除之后，这个pod对应的PVC和PV，并不会被删除，而这个volume里已经写入的数据，也依然会保存在远程存储服务里（比如，ceph服务器）

此时，StatefulSet控制器发现，一个名叫web-0的pod消失了，所以，控制器就会重新创建一个新的，名字还是叫做web-0的pod来，“纠正”这个不一致的情况

需要注意的是，在这个新的pod对象的定义里，它声明使用的PVC的名字，还是叫做：www-web-0.这个PVC的定义还是来自于PVC模板（volumeClaimTemplates）,这是StatefulSet创建pod的标准流程

所以，在这个新对额web-0 pod被创建出来后，k8s为它查找名叫www-web-0的PVC时，就会直接找到旧pod遗留下来的同名的PVC，进而找到跟这个PVC绑定在一起的PV


这样，新的pod就可以挂载到旧Pod对应的哪个Volume，并且获取到保存在volume里的数据

通过这种方式，k8s的StatefulSet就实现了对应用存储状态的管理



详细梳理：
首先，StatefulSet的控制器直接管理的是pod,这是因为，StatefulSet里不同pod实例，不再像ReplicaSet中那样都是完全一样的，而是有了细微区别，比如，每个pod中的hostname、名字都是不同的，携带了编号的。而statefulSet区分这些实例的方式，就是通过Pod的名字里加上事先约定号的编号


其次，k8s通过headless Service，为这些有编号的pod，再DNS服务器中生成带有同样编号的DNS记录，只要StatefulSet能够保证这些Pod名字里的编号不变，那么Servcice里类似于web-0.nginx.default,svc.cluster.local这样的DNS记录也会不会变，而这条记录解析出来的pod的IP地址，则会随着后端pod的删除和再创建而自动创新。这当然是service机制本身能力，不需要StatefulSet操心


最后，StatefulSet还未每一个pod分配并创建一个同样编号的PVC，这样，k8s就可以通过persistent volume机制未这个PVC，绑定对应的PV，从而保证了每一个pod都拥有一个独立的volume
在这种情况下，即使pod被删除，它所对应的PVC和PV依然会保留下来，所以，当这个pod被重新创建出来之后，k8s会未它找到同样编号的PVC，挂载这个PVC对应的volume，从而获取到以前保存在volume里的数据



	
