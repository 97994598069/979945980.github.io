red

原因
red表示不是所有的主分片都可用，通常时由于某个索引的住分片为分片unassigned，只要找出这个索引的分片，手工分配即可

处理
官方文档的详细说明
通过curl GET http://{ESIP}:9200/_cluster/health?level=indices查看所有索引信息，查看下是哪个索引的status是red导致了集群都red了(集群索引多的时候一眼看不出来，直接把结果拷出来，搜red关键字就跟踪到索引和分片了)
如果这个索引不是很重要，直接delete掉省事，集群状态马上恢复green完事~
通过reroute强制分配该分片（文章后面）


yellow

原因
yellow表示所有主分片可用，但不是所有副本分片都可用，最常见的情景是单节点时，由于es默认是有1个副本，主分片和副本不能在同一个节点上，所以副本就是未分配unassigned

处理
过滤查看所有未分配索引的方式，curl -s "http://10.19.22.142:9200/_cat/shards" | grep UNASSIGNED结果如下，第一列表示索引名，第二列表示分片编号，第三列p是主分片，r是副本
curl -s "http://{ESIP}:9200/_cat/shards" | grep UNASSIGNED
eslog1                 3 p UNASSIGNED
eslog1                 3 r UNASSIGNED
eslog1                 1 p UNASSIGNED
eslog1                 1 r UNASSIGNED



分配分片
知道哪个索引的哪个分片就开始手动修复，通过reroute的allocate分配
curl -XPOST '{ESIP}:9200/_cluster/reroute' -d '{
    "commands" : [ {
          "allocate" : {
              "index" : "eslog1",
              "shard" : 4,
              "node" : "es1",
              "allow_primary" : true
          }
        }
    ]
}'


分配时可能遇到的坑，需要注意的地方
分配副本时必须要带参数"allow_primary" : true, 不然会报错
当集群中es版本不同时，如果这个未分配的分片是高版本生成的，不能分配到低版本节点上，反过来低版本的分片可以分配给高版本，如果遇到了，只要升级低版本节点的ES版本即可（博主注：es的版本号要全部一致，小版本之间也会出现此兼容性问题，比如2.4.4和2.4.6，并且升级es要谨慎，也是一个大坑）
（升级ES版本详见官方详细文档，我是ubuntu系统apt安装的，直接apt-get install elasticsearch升级的，elasticsearch.yml配置文件没变不用修改，但是/usr/share/elasticsearch/bin/elasticsearch文件中有个内存配置ES_HEAP_SIZE=6G需要再手动加一下&重启es）


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ES 集群不健康red解决办法

单节点yellow状态:
大量unassigned shards
其实刚搭完运行时就是status: yellow(所有主分片可用，但存在不可用的从分片), 只有一个节点, 主分片启动并运行正常, 可以成功处理请求, 但是存在unassigned_shards, 即存在没有被分配到节点的从分片.(只有一个节点.....)
.当时数据量小, 就暂时没关注. 然后, 随着时间推移, 出现了大量unassigned shards

curl -XGET http://localhost:9200/_cluster/health\?pretty
{
  "cluster_name" : "elasticsearch",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 538,
  "active_shards" : 538,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 558,
"number_of_pending_tasks" : 0
}
处理方式: 找了台内网机器, 部署另一个节点(保证cluster.name一致即可, 自动发现, 赞一个). 当然, 如果你资源有限只有一台机器, 使用相同命令再启动一个es实例也行. 再次检查集群健康, 发现unassigned_shards减少, active_shards增多.
操作完后, 集群健康从yellow恢复到 green


status: red
这次检查发现是status: red(存在不可用的主要分片)
curl -XGET http://localhost:9200/_cluster/health\?pretty
{
  "cluster_name" : "elasticsearch",
  "status" : "red",    // missing some primary shards
  "timed_out" : false,
  "number_of_nodes" : 4,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 538,
  "active_shards" : 1076,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 20,  // where your lost primary shards are.
  "number_of_pending_tasks" : 0
}


开始着手修复
查看所有分片状态
curl -XGET http://localhost:9200/_cat/shards


找出UNASSIGNED分片
curl -s "http://localhost:9200/_cat/shards" | grep UNASSIGNED
pv-2015.05.22                 3 p UNASSIGNED
pv-2015.05.22                 3 r UNASSIGNED
pv-2015.05.22                 1 p UNASSIGNED
pv-2015.05.22                 1 r UNASSIGNED


查询得到master节点的唯一标识
curl 'localhost:9200/_nodes/process?pretty'

{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "AfUyuXmGTESHXpwi4OExxx" : {
      "name" : "Master",
     ....
      "attributes" : {
        "master" : "true"
      },
.....



执行reroute(分多次, 变更shard的值为UNASSIGNED查询结果中编号, 上一步查询结果是1和3)
curl -XPOST 'localhost:9200/_cluster/reroute' -d '{
        "commands" : [ {
              "allocate" : {
                  "index" : "pv-2015.05.22",
                  "shard" : 1,
                  "node" : "AfUyuXmGTESHXpwi4OExxx",
                  "allow_primary" : true
              }
            }
        ]
    }'
	
	

批量处理的脚本(当数量很多的话, 注意替换node的名字)
#!/bin/bash

for index in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | awk '{print $1}' | sort | uniq); do
    for shard in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | grep $index | awk '{print $2}' | sort | uniq); do
        echo  $index $shard

        curl -XPOST 'localhost:9200/_cluster/reroute' -d "{
            'commands' : [ {
                  'allocate' : {
                      'index' : $index,
                      'shard' : $shard,
                      'node' : 'Master',
                      'allow_primary' : true
                  }
                }
            ]
        }"

        sleep 5
    done
done



“Too many open files”
发现日志中大量出现这个错误
执行
curl http://localhost:9200/_nodes/process\?pretty

可以看到
"max_file_descriptors" : 4096,

官方文档中
Make sure to increase the number of open files descriptors on the machine (or for the user running elasticsearch). Setting it to 32k or even 64k is recommended.

而此时, 可以在系统级做修改, 然后全局生效


最简单的做法, 在bin/elasticsearch文件开始的位置加入
ulimit -n 64000


然后重启es, 再次查询看到
"max_file_descriptors" : 64000,


+++++++++++++++++++++++++++++++++++++++++++++

ELASTICSEARCH健康red的解决
1.检查配置项recover_after_data_nodes: 3     #data nodes 的节点数，如果超出实际节点数，active_shards_percent： %NaN
访问 /_cat/shards 会报 blocked by: [SERVICE_UNAVAILABLE/1/state not recovered /错误，并且head 无法连接es 集群，集群状态为 red。


curl http://metron01:9200/_cat/health?v
 

epoch      timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent 
1534901569 09:32:49  metron  green           3         2      2   1    0    0        0             0                  -                NaN% 


2.删除索引目录下的文件
查看配置elasticsearch.yml中的data 目录
data: "/home/opt/lmm/es_data"
# 清空所有节点上的该目录下

rm -rf /home/opt/lmm/es_data/*
重启服务


3、shards存在UNASSIGNED
# 查询 UNASSIGNED 的索引名称和序号
curl -s "http://localhost:9200/_cat/shards" | grep UNASSIGNED
snort_index_2018.08.17.15 2 p UNASSIGNED                   
snort_index_2018.08.17.15 2 r UNASSIGNED                                     
snort_index_2018.08.17.15 1 p UNASSIGNED                                     
snort_index_2018.08.17.15 1 r UNASSIGNED                                     
snort_index_2018.08.17.15 3 p UNASSIGNED                                     
snort_index_2018.08.17.15 3 r UNASSIGNED                                     
snort_index_2018.08.17.15 0 p UNASSIGNED                                     
snort_index_2018.08.17.15 0 r UNASSIGNED

#索引名 snort_index_2018.08.17.15 序号为 0,1,2,3


查询data node 的ID

curl 'localhost:9200/_nodes/process?pretty'

{

  "cluster_name" : "metron",

  "nodes" : {

    "I3_T2xI1RGCocy7sZcdg9w" : {

      "name" : "metron03",

      "transport_address" : "172.16.16.59:9300",

      "host" : "172.16.16.59",

      "ip" : "172.16.16.59",

      "version" : "2.3.3",

      "build" : "218bdf1",

      "http_address" : "172.16.16.59:9200",

      "attributes" : {

        "master" : "false"

      },

      "process" : {

        "refresh_interval_in_millis" : 1000,

        "id" : 16002,

        "mlockall" : false

      }

    },

    "xOk36eUVQhSgAHYv6VT77Q" : {

      "name" : "metron01",

      "transport_address" : "172.16.16.57:9300",

      "host" : "172.16.16.57",

      "ip" : "172.16.16.57",

      "version" : "2.3.3",

      "build" : "218bdf1",

      "http_address" : "172.16.16.57:9200",

      "attributes" : {

        "data" : "false",

        "master" : "true"

      },

      "process" : {

        "refresh_interval_in_millis" : 1000,

        "id" : 25044,

        "mlockall" : false

      }

    },

    "ncgAeS6nQp-L0d3U0roVMA" : {

      "name" : "metron02",

      "transport_address" : "172.16.16.58:9300",

      "host" : "172.16.16.58",

      "ip" : "172.16.16.58",

      "version" : "2.3.3",

      "build" : "218bdf1",

      "http_address" : "172.16.16.58:9200",

      "attributes" : {

        "master" : "false"

      },

      "process" : {

        "refresh_interval_in_millis" : 1000,

        "id" : 12653,

        "mlockall" : false

      }

    }

  }

}


针对每个shard 和序号 执行reroute

curl -XPOST 'metron01:9200/_cluster/reroute' -d '{

        "commands" : [ {

              "allocate" : {

                  "index" : "snort_index_2018.08.17.15",  # 索引名称

                  "shard" : 0,   # shard ID

                  "node" : "I3_T2xI1RGCocy7sZcdg9w", # data node ID

                  "allow_primary" : true

              }

            }

        ]

    }'

curl -XPOST 'metron01:9200/_cluster/reroute' -d '{

        "commands" : [ {

              "allocate" : {

                  "index" : "snort_index_2018.08.17.15",  

                  "shard" : 1,   

                  "node" : "I3_T2xI1RGCocy7sZcdg9w",

                  "allow_primary" : true

              }

            }

        ]

    }'

curl -XPOST 'metron01:9200/_cluster/reroute' -d '{

        "commands" : [ {

              "allocate" : {

                  "index" : "snort_index_2018.08.17.15",  

                  "shard" : 2,  

                  "node" : "I3_T2xI1RGCocy7sZcdg9w", 

                  "allow_primary" : true

              }

            }

        ]

    }'

curl -XPOST 'metron01:9200/_cluster/reroute' -d '{

        "commands" : [ {

              "allocate" : {

                  "index" : "snort_index_2018.08.17.15",  

                  "shard" : 3,   

                  "node" : "I3_T2xI1RGCocy7sZcdg9w", 

                  "allow_primary" : true

              }

            }

        ]

    }'


如果出现如下错误：

{

	"error": {

		"root_cause": [

			{

				"type": "illegal_argument_exception",

				"reason": "[allocate] allocation of [.kibana][0] on node {metron03}{WPJIoZ6CRTetEAH_j8nPBw}{172.16.16.59}{172.16.16.59:9300}{master=false} is not allowed, reason: [YES(primary is already active)][YES(allocation disabling is ignored)][YES(target node version [2.3.3] is same or newer than source node version [2.3.3])][YES(node passes include/exclude/require filters)][YES(total shard limit disabled: [index: -1, cluster: -1] <= 0)][YES(no allocation awareness enabled)][YES(enough disk for shard on node, free: [187.9gb])][YES(allocation disabling is ignored)][YES(shard not primary or relocation disabled)][YES(below shard recovery limit of [4])][NO(shard cannot be allocated on same node [WPJIoZ6CRTetEAH_j8nPBw] it already exists on)]"

			}

		],

		"type": "illegal_argument_exception",

		"reason": "[allocate] allocation of [.kibana][0] on node {metron03}{WPJIoZ6CRTetEAH_j8nPBw}{172.16.16.59}{172.16.16.59:9300}{master=false} is not allowed, reason: [YES(primary is already active)][YES(allocation disabling is ignored)][YES(target node version [2.3.3] is same or newer than source node version [2.3.3])][YES(node passes include/exclude/require filters)][YES(total shard limit disabled: [index: -1, cluster: -1] <= 0)][YES(no allocation awareness enabled)][YES(enough disk for shard on node, free: [187.9gb])][YES(allocation disabling is ignored)][YES(shard not primary or relocation disabled)][YES(below shard recovery limit of [4])][NO(shard cannot be allocated on same node [WPJIoZ6CRTetEAH_j8nPBw] it already exists on)]"

	},

	"status": 400

}



可以通过设置副本数为0，es 自动删除多余的副本，再改回2，来处理：

curl -XPUT  http://metron01:9200/.kibana/_settings -d'{

    "index" : {

        "number_of_replicas" : 0

    }

}'

返回：{"acknowledged":true}

curl -XPUT  http://metron01:9200/.kibana/_settings -d'{

    "index" : {

        "number_of_replicas" : 2

    }

}'

返回：{"acknowledged":true}














